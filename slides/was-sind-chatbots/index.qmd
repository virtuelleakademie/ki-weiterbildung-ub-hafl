---
title: "Wie funktionieren Chatbots?"
---

## Was sind Chatbots?

:::: {.columns}
::: {.column width="30%"}

:::

::: {.column width="70%"}
![](../../assets/images/claude.png)
:::
::::

## Was sind LLMs?

:::: {.columns}
::: {.column width="60%"}
Ein LLM kann man sich wie einen ausgefeilten Autocomplete-Mechanismus vorstellen.
:::
::: {.column width="40%"}
![](../../assets/images/predictive-text.png)
:::
::::

:::{.attribution}
Bildquelle: [www.apple.com](https://www.apple.com)
:::



## Was sind LLMs?
- Statistische Modelle, die Text analysieren, um das nächste Wort vorherzusagen.

::: {.hidden}
$$
\newcommand{\green}[1]{\color{green}{#1}}
\newcommand{\purple}[1]{\color{purple}{#1}}
\newcommand{\red}[1]{\color{red}{#1}}
\newcommand{\blue}[1]{\color{blue}{#1}}
$$
:::

$$\green{P(\text{Wort}_{i+1}} \mid \red{\text{Kontext}}, \blue{\text{Modell}})$$



- Jede $\purple{\text{Vorhersage}}$ basiert auf dem $\blue{\text{Kontext}}$ und dem internen $\red{\text{Modell}}$.



## Vorhersage

Nicht alle Teile des Kontexts sind gleich wichtig:

<br> <br>

<!-- :::{.callout-note appearance="minimal"}-->
:::{.r-fit-text}
_"Die Familie, die sehr wohlhabend war, lebte in einem grossen Haus. Das Haus stand inmitten eines weitläufigen Gartens. Es war bekannt für seine prächtige Fassade und die grosszügigen ___"_
::: 

:::{.attribution}
Nach Thomas Mann, *Buddenbrooks*
:::

<br> <br>

Welche Wörter sind besonders wichtig, um 

- die Bedeutung des Satzes zu erfassen?
- das nächste Wort vorherzusagen?


## Kontext verstehen {.smaller}

:::{.callout-note appearance="minimal"}
"Die Familie, die sehr wohlhabend war, lebte in einem grossen Haus. Das Haus stand inmitten eines weitläufigen Gartens. Es war bekannt für seine prächtige Fassade und die grosszügigen ___"
:::

**Syntaktische Struktur (Grammatik und Struktur des Satzes):**

- Das Wort "grosszügigen" ist ein Adjektiv, das wahrscheinlich ein Nomen---im Plural---beschreibt (Dativ oder Akkusativ wegen der Endung "-en").
- Der Satz bezieht sich auf das Haus und den Garten, daher liegt der Fokus vermutlich auf deren Eigenschaften.

**Semantischer Kontext (Bedeutung):**

Die Beschreibung hebt Wohlstand hervor. Das nächste Wort beschreibt vermutlich etwas Luxuriöses oder Weitläufiges.

**Lexikalische Kohärenz (Wörter und deren Bedeutungen im Kontext):**

Nach "grosszügigen" folgen häufig Nomen, die Räume, Flächen oder architektonische Elemente beschreiben, z. B. "Räume", "Gärten", "Fenster".





<!-- ## Wie generieren LLMs Text?

<br> <br>

![](../../assets/images/wort-fuer-wort.png) -->


## Wie generieren LLMs Text?

<br> <br>

::: {.hidden}
$$
\newcommand{\green}[1]{\color{green}{#1}}
\newcommand{\purple}[1]{\color{purple}{#1}}
\newcommand{\red}[1]{\color{red}{#1}}
\newcommand{\blue}[1]{\color{blue}{#1}}
$$
:::

$$\green{P(\text{Wort}_{i+1}} \mid \red{\text{Kontext}}, \blue{\text{Modell}})$$



![](../../assets/images/hf-is-a-startup.gif)




<!-- ## Wie können LLMs Text vorhersagen? {.smaller}

Sie werden trainiert, das nächste Wort in einer gegebenen Wortsequenz zu erraten.

:::: {.columns}
::: {.column width="50%"}
Ein LLM wird in drei Schritten aufgebaut:

1. Sammeln eines grossen Text-Korpus.
2. Basierend auf diesem Text, muss das Modell das nächste Wort in einer gegebenen Wortsequenz vorherzusagen lernen.
3. Das Sprachmodell wird feiner abgestimmt, um das gewünschte Verhalten zu erreichen.

:::
::: {.column width="50%"}
![](../../assets/images/LLM-Bookshelf.png)
:::
:::: -->


## Wie werden LLMs trainiert?

![](../../assets/images/llm-training.png)


## Extended Thinking: Wenn LLMs nachdenken {.smaller}

**Von Textgenerierung zu Reasoning**
Neuere Modelle (o0/o3, GPT-5,  Claude Opus/Sonnet/Haiku) können "nachdenken" bevor sie eine Antwort generieren:

1. **Problem analysieren**: Aufgabe in Teilschritte zerlegen
2. **Optionen durchdenken**:Verschiedene Ansätze prüfen
3. **Reasoning-Kette aufbauen**: Schritte logisch verbinden
4. **Antwort generieren**: Basierend auf Thinkink-Output

**Vorteil:** Bessere Antworten bei komplexen Aufgaben (Planung, Problemlösung, Mathematik)

**Nachteil:** Langsamer und teurer als Standard-Modus

<br>

::::{.columns}
:::{.column width="50%"}

### Standard-Modus
**Frage** → **Antwort**

*Schnell, direkt*
:::

:::{.column width="50%"}

### Thinking-Modus
**Frage** → **<denkt nach...>** → **Antwort**

*Langsam, gründlich*
:::
::::



## Gefahren und Herausforderungen

:::: {.columns}
::: {.column width="50%"}
:::{.r-fit-text}
Die verschiedenen Stufen des Trainings sind mit verschiedenen Arten von Bedenken verbunden:

- **Urheberrecht**: Die trainierten Modelle werden mit Texten trainiert, die möglicherweise Urheberrechtlich geschützt sind.
- **Bias**: Die trainierten Modelle können bestehende Vorurteile aus den Trainingsdaten lernen.
- **Energieverbrauch**: Das Training der Modelle verbraucht viel Energie und ist damit umweltbelastend.
- **Sycophancy**: Die Modelle neigen dazu, die Meinungen oder Präferenzen ihrer Benutzer zu bestätigen.
:::

:::
::: {.column width="50%"}
![](../../assets/images/Pothole.png)
:::
::::

## Gefahren und Herausforderungen

:::: {.columns}
::: {.column width="50%"}
:::{.r-fit-text}
- Obschon sich LLMs viel Wissen aneignen[^1], werden sie nicht trainiert, faktisch korrekte Aussagen zu machen.
- Dies bedeutet, dass wir alle Aussagen, die LLMs uns präsentieren, immer kritisch hinterfragen müssen.
- LLMs sind **keine Wissensdatenbanken**. Informationen immer anhand externer Quellen überprüfen.
:::
:::
::: {.column width="50%"}
![](../../assets/images/KnowledgeBase.png)
:::
::::

[^1]: Das ganze Wissen, welches nötig ist, um Texte Wort für Wort vorherzusagen.




## ChatGPT


![](../../assets/images/chatgpt.png)



## Fragen beantworten

![](../../assets/images/fragen-beantworten.png)


## Bilder analysieren

![](../../assets/images/bilder-analysieren.png)

## Dokumente zusammenfassen

![](../../assets/images/dokumente-zusammenfassen.png)

## Output strukturieren

![](../../assets/images/output-strukturieren.png)

## Websuche

![](../../assets/images/websuche.png)

## Datenanalyse

![](../../assets/images/datenanalyse.png)


## Custom GPTs

![](../../assets/images/gpts.png)


## Der Paradigmenwechsel {.smaller}


::::{.columns}
:::{.column width="50%"}
### Traditional Chatbot
**Benutzer**: "Plane eine Reise nach Paris"

**Chatbot**:

- "Hier sind 5 Hotels in Paris..."
- "Diese Flüge sind verfügbar..."
- "Sehenswürdigkeiten: Eiffelturm..."
- **Du musst alles selbst buchen**
:::

:::{.column width="50%"}
### AI Agent
**Benutzer**: "Plane eine Reise nach Paris"

**Agent**:

- Checkt deinen Kalender
- Bucht Flug und Hotel
- Erstellt Reiseroute
- Sendet Bestätigungen
- **Erledigt die komplette Aufgabe**
:::
::::

::: {.callout-important}
**Der Unterschied**: Ein Chatbot kann eine Frage beantworten. Ein Agent bearbeitet das Problem, bis der Job erledigt ist.
:::


## Was sind AI Agents? {.smaller}

**LLM + Gedächtnis + Werkzeuge + Entscheidungsschleife**

::::{.columns}
:::{.column width="50%"}

**LLM-basierte Agenten** =

- **LLM** (GPT-5, Claude, ...) für Reasoning
- **+ Gedächtnis**
- **+ Werkzeuge (Tools)** (APIs, Datenbanken, Code)
- **+ Entscheidungsschleife** (kontinuierlich)

:::

::: {.column width="50%"}
### Der Agent-Zyklus

1. **Wahrnehmung**: Input/Umgebung
2. **Planung**: LLM überlegt Schritte
3. **Handlung**: Tools/APIs verwenden
4. **Feedback**: Ergebnisse bewerten

:::
::::



## Von Chatbot zu Agent: Unterschiedie {.smaller}

**Zwei verschiedene Paradigmen**

| Dimension | **Chatbot** | **AI Agent** |
|-----------|-------------|--------------|
| **Verhalten** | • Reaktiv: Antwortet nur auf Anfragen | • Proaktiv: Arbeitet kontinuierlich zum Ziel |
| **Aufgaben** | • Einzelne Antworten<br>• Session-basiert | • Komplette Workflows<br>• End-to-End Execution |
| **Gedächtnis** | • Nur während Konversation (im Kontext)<br>• Kein Lernen | • Persistent über Zeit<br>• Lernt aus Interaktionen (speichert "Memories") |
| **Tools** | • Keine externen Integrationen | • APIs, Datenbanken, Apps<br>• Real-time Integration |
| **Planung** | • Keine Strategieentwicklung | • Multi-step Planning<br>• Selbstkorrektur |



## ChatGPT: GPTs

:::: {.columns}
::: {.column width="50%"}
![](../../assets/images/gpts-1.png)
:::
::: {.column width="50%"}
![](../../assets/images/gpts-2.png)
:::
::::

## Copilot Agents

:::: {.columns}
::: {.column width="50%"}
![](../../assets/images/copilot-1.png)
:::
::: {.column width="50%"}
![](../../assets/images/copilot-2.png)
:::
::::

